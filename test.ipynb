{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training yolov8 model \n",
    "The following script is used to train a yolov8 model. \n",
    "\n",
    "Use this script in google colab pro + to train model otherwise training will be really slow. Copy and paste this code into a google collab enviroment. Make sure to specify the model and dataset. See yolov8 docs for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training script for yolov8 model. \n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n",
    "\n",
    "model = YOLO(\"runs/detect/train53/weights/last.pt\")\n",
    "\n",
    "# Use the model\n",
    "# Make sure the data parameter points to the data.yaml file. The data.yaml file can be found in the datasets folder on sharepoint.\n",
    "model.train(data=\"data.yaml\", epochs=100, device= \"0\", batch= -1)  # train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using trained model\n",
    "\n",
    "Once a model is trained, use the model on a test drawing package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the model\n",
    "model = YOLO('yoloModels/collab3_last.pt')\n",
    "\n",
    "def process_images(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Run the YOLO model on all PNG images in the input folder and save the results in the output folder.\n",
    "\n",
    "    :param input_folder: Path to the folder containing input PNG images.\n",
    "    :param output_folder: Path to the folder to save output images.\n",
    "    \"\"\"\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate over all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith('.png'):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Run inference on the image\n",
    "            results = model(input_path)\n",
    "\n",
    "\n",
    "            # Process results and save the output image\n",
    "            for result in results:\n",
    "                result.save(filename=output_path)\n",
    "                print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"testTif\"\n",
    "output_folder = \"test_colab3\"\n",
    "process_images(input_folder, output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dimensions\n",
    "Use  OpenAI to run OCR on the dimensions of the image. This function reads the value from an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"TMG_OpenAI_API\"))\n",
    "\n",
    "\n",
    "def readNumber (base64_image):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"This image might have a number in it. Can you tell me what number it is and only that number? if there is no number can you respond with no number. Numbers maybe rotated, if the numbers are rotated can you respond with no number?\"},\n",
    "            {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                \n",
    "            },\n",
    "            },\n",
    "        ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    "    )\n",
    "\n",
    "    # print(response.choices[0].message.content)\n",
    "    return (response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ouputing the dimension values\n",
    "\n",
    "Use the model to read a drawing to locate all the coordinates of all the dimensions  \n",
    "Read all the dimensions by calling the above function.  \n",
    "Put all the values into an array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import base64\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('yoloModels/colab_2_last.pt')\n",
    "\n",
    "# Load the image\n",
    "image_path = 'test/22028-D-254-0_page_1.png'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Perform inference\n",
    "results = model(image)\n",
    "dimensions = []\n",
    "\n",
    "def is_float(value):\n",
    "    \"\"\" Check if the value is an integer. \"\"\"\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def encode_image(image_path):\n",
    "    _, buffer = cv2.imencode(\".jpg\", image_path)\n",
    "    return base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "# View results\n",
    "for r in results:\n",
    "    boxes = r.boxes.cpu().numpy()    # print the Boxes object containing the detection bounding boxes\n",
    "\n",
    "    XYcoords = boxes.xyxy\n",
    "    dimClass = boxes.cls\n",
    "    # print(XYcoords)\n",
    "    # print(dimClass)\n",
    "\n",
    "    # Iterate through the array of bounding boxes\n",
    "    dimCount = 1\n",
    "    for i, (xmin, ymin, xmax, ymax) in enumerate(XYcoords, 1):\n",
    "        \n",
    "        if dimClass[i-1] == 0:\n",
    "        \n",
    "            # Convert coordinates to integers (if they are not already)\n",
    "            xmin, ymin, xmax, ymax = map(int, (xmin, ymin, xmax, ymax))\n",
    "\n",
    "            # Crop the region of interest (ROI) from the image\n",
    "            roi = image[ymin:ymax, xmin:xmax]\n",
    "\n",
    "            # Resize image \n",
    "            resized_image = cv2.resize(roi, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n",
    "            # print(\"Image shape:\", resized_image.shape)\n",
    "\n",
    "            #do thresholding\n",
    "            gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "            _, thresholded_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "                    #general label\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "\n",
    "            # # Define label position (slightly above the top-left corner of the bounding box)\n",
    "            label_position_gen = (xmin, ymin - 10) if ymin - 10 > 10 else (xmin, ymin + 20)\n",
    "            label_gen = f'{i}'\n",
    "            print(label_gen)\n",
    "\n",
    "            # Add label to the image\n",
    "            cv2.putText(image, label_gen, label_position_gen, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "            # encode image\n",
    "            encoded_img = encode_image(thresholded_image)\n",
    "\n",
    "            # print the value that openai reads\n",
    "            \n",
    "            dim = readNumber(encoded_img)\n",
    "            \n",
    "            try:\n",
    "            # Try to convert the result to a float\n",
    "                number = float(dim)\n",
    "                print(number)\n",
    "                dimensions.append(number)  # Append to the array if successful\n",
    "            except ValueError:\n",
    "            # Handle the case where dim is not a number (e.g., it's a string)\n",
    "                print(f\"Value returned is not a number: {dim}\")\n",
    "                # dimensions.append(f\"{label_gen}_n/a\")\n",
    "\n",
    "            # rotating the image\n",
    "\n",
    "                rotatedImg = cv2.rotate(thresholded_image, cv2.ROTATE_90_CLOCKWISE)\n",
    "                encoded_img_rot = encode_image(rotatedImg)\n",
    "\n",
    "                dim_rotate = readNumber(encoded_img_rot)\n",
    "                dimensions.append(dim_rotate)\n",
    "\n",
    "\n",
    "            # Print or store the extracted text\n",
    "            # print(f\"Box {i}: {text.strip()}, Rotated: {text_rotated}\")\n",
    "\n",
    "    \n",
    "            cv2.waitKey(0)  # Wait indefinitely until a key is pressed\n",
    "    cv2.destroyAllWindows()  # Close the window\n",
    "        \n",
    "\n",
    "    # Save the annotated image\n",
    "    output_path = 'labeled_image2.png'\n",
    "    print(dimensions)\n",
    "    cv2.imwrite(output_path, image)\n",
    "\n",
    "    print(f\"Image saved with labels as {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
